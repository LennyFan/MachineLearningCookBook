{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA Analysis\n",
    "\n",
    "     **is a transformation which attempts to find out what features explain the most variance in the data.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Directions of Maximum Variance\n",
    "\n",
    "> ** Q&A: **\n",
    "\n",
    "> - **What is the direction of maximum variance and how to get it?**\n",
    "\n",
    "> - **Why it's related to eigen value, and eigen vector?**\n",
    "\n",
    "> - **What is the 1st Principal Component?**\n",
    "\n",
    "> - **How to get 2nd Principal Component?**\n",
    "\n",
    "> - **PCA in general **\n",
    "\n",
    "> - **What is New Coordiante System? **\n",
    "\n",
    "> - **Sample Variance Note**\n",
    "\n",
    "> - **Using variance to explain the data**\n",
    "\n",
    "***\n",
    "\n",
    "$$$$\n",
    "\n",
    "### What is the direction of maximum variance and how to get it? \n",
    "\n",
    "\n",
    "The goal is finding a vector $\\hat a \\in \\mathbb{R}^p$, where the collection of $n$ data points' scalar projection on $\\hat a$ have the largest sample variance amonog other vecter in $\\mathbb{R}^p$ space ( p is the number of parameters )\n",
    "\n",
    "For the convience, let's assume we centralize the data point ( $X\\in\\mathbb{R}^{p\\times n} $ with 0 sample mean for each feature coordinates )\n",
    "\n",
    "---\n",
    "\n",
    "$$ \\therefore \\;\\; \\displaystyle \\mathbb{E}[a^T X] = \\hat{0}_n^T \\;\\; \\mbox{  ( zero row vector ) } $$\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "$$ \\therefore \\;\\;  \\displaystyle \\mathbb{Var}[a^T X] = \\mathbb{E}\\left[(a^T X - E[a^T X])(a^T X - E[a^T X])^T\\right] = \\mathbb{E}\\left[(a^T X) (a^T X)^T\\right] $$\n",
    "\n",
    "$$ = \\mathbb{E}\\left[a^T (X X^T) a\\right] = a^T\\,\\mathbb{E}\\!\\left[XX^T\\right]\\,a $$\n",
    "\n",
    "\n",
    "Since the covariance matrix of $X$ is \n",
    "\n",
    "$$ C = \\displaystyle \\mathbb{Var}[X]  =  \\mathbb{E}\\left[(X - \\mathbb{E}[X] ) (X - \\mathbb{E}[X] )^T\\right] = \\mathbb{E}\\left[X X^T\\right] $$\n",
    "\n",
    "Thus we want to maximize\n",
    "\n",
    "$$ \\displaystyle \\max_{a \\in \\mathbb{R}^p} \\mathbb{Var}[a^T X] = \\max_{a \\in \\mathbb{R}^p} a^T C\\,a $$\n",
    "\n",
    "---\n",
    "\n",
    "However, if there is no constraint for $\\hat a$, the variance can be arbitrarily large by assigning large absolute value in $\\hat a$. Therefore, we would like to set a constraint on $\\hat a$ to let $\\hat a$ to be unit vector, such that $\\hat a^2 = 1$.\n",
    "\n",
    "In other words,\n",
    "\n",
    "$$ \\max_{a \\in \\mathbb{R}^p} a^T C\\,a \\\\ \\mbox{s.t.} \\;\\;a^2 = a^T a = 1 $$\n",
    "\n",
    "---\n",
    "\n",
    "$$$$\n",
    "\n",
    "### Why it's related to eigen value, and eigen vector? \n",
    "\n",
    "The constraint optimization problem can be solved by the **Lagrange multiplier** with extra term $\\lambda \\in \\mathbb{R}$\n",
    "\n",
    "$$\\displaystyle  \\max_{a\\in{}\\mathbb{R}^p, \\; \\lambda \\in \\mathbb{R} } \\left[a^T C\\,a - \\lambda\\,(a^2\\!-\\!1)\\right]$$\n",
    "\n",
    "> Note: \n",
    "\n",
    "> 1. the **Lagrange multiplier** will have feasible solution iff $a^T a = 1 \\\\\\\\\\\\\\\\$\n",
    "\n",
    "> 2. $$ f(x) = X^TAX + bX $$ \n",
    "\n",
    "> $$df(x) = (A^T + A)X + b $$\n",
    "\n",
    "Thus, since the original euqation is quadratic function, \n",
    "\n",
    "we will have global maximum when the partial derivative with repect to $\\hat a$ is 0\n",
    "\n",
    "$$  \\displaystyle \\frac{\\partial}{\\partial a} =  Ca + C^Ta - 2\\lambda a = 0$$\n",
    "\n",
    "\n",
    "$$ \\displaystyle \\frac{\\partial}{\\partial \\lambda} =  a^2 - 1 = 0 $$\n",
    "\n",
    "---\n",
    "\n",
    "**But $C$ is symmetric, it turns out that it is actually a eigendecoposition problem !! ( [Eigenvalue and Eigenvector](https://www.youtube.com/watch?v=PFDu9oVAE-g&index=14&list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab) )**\n",
    "\n",
    "$$\\displaystyle  Ca = \\lambda a $$\n",
    "\n",
    "\n",
    "### What is the 1st Principal Component?\n",
    "\n",
    "By the equation above, the value of maximum variance is\n",
    "\n",
    "\n",
    "$$\\therefore \\;\\;\\;\\displaystyle \\max_{a \\in \\mathbb{R}^p} a^TCa =  a^T \\lambda a =  \\lambda a^Ta =  \\max_{a\\in{}\\mathbb{R}^p, \\; \\lambda \\in \\mathbb{R} }\\lambda $$ \n",
    "\n",
    "$\\\\$\n",
    "\n",
    "\n",
    "It turns out that the **largest eigenvalue $\\lambda_1$ corresponding eigenvector $a_1$** is the **1st Principal Component** \n",
    "\n",
    "\n",
    "$$\\displaystyle  Ca_1 = \\lambda_1 a_1 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to get 2nd Principal Component?\n",
    "\n",
    "        **Direction of largest variance uncorrelated to 1st PC**\n",
    "\n",
    "$$\\displaystyle  \\max_{a\\in{}\\mathbb{R}^N} \\left[a^T C\\,a - \\lambda\\,(a^2\\!-\\!1) - \\lambda'(a^T C\\,a_1) \\right]$$\n",
    "\n",
    "Partial derivatives vanish at optimum, thus\n",
    "\n",
    "$$\\displaystyle 2Ca - 2\\lambda{}a-\\lambda'Ca_1 = 0$$\n",
    "\n",
    "--- \n",
    "Multiply above equation by $a_1^T$, we will get\n",
    "\n",
    "$$\\displaystyle 2a_1^TCa - 2a_1^T\\lambda{}a-a_1^T\\lambda'Ca_1 = 0\\\\$$\n",
    "\n",
    "$$\\displaystyle 0 - 0 - \\lambda'\\lambda_1 = 0 \\ \\ \\rightarrow\\ \\  \\lambda'=0$$\n",
    "\n",
    "**Still just an eigenproblem **\n",
    "\n",
    "$$\\displaystyle  Ca = \\lambda a $$\n",
    "\n",
    "The solution is therefore $\\lambda_2$ and $a_2$ which is second largest eigenvalue and eigenvector ( **2nd Principal Component** )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### PCA in general\n",
    "\n",
    "> Let $\\lambda_1\\geq\\lambda_2\\geq\\dots\\geq\\lambda_N\\geq{}0$ be the eigenvalues of $C$ and $\\hat{e}_1,\\dots,\\hat{e}_N$ the corresponding eigenvectors\n",
    "\n",
    "> $\\displaystyle  C = \\sum_{k=1}^N\\ \\lambda_k\\left(\\hat{e}_k\\,\\hat{e}_k^T\\right) $\n",
    "\n",
    "> With diagonal $\\Lambda$ matrix of the eigenvalues and an $E$ matrix of $[\\hat{e}_1, \\dots, \\hat{e}_N]$\n",
    "\n",
    "> $\\displaystyle  C = E\\ \\Lambda\\ E^T$\n",
    "\n",
    "\n",
    "- The eigenvectors of largest eigenvalues capture the most variance\n",
    "\n",
    "> If keeping only $K<N$ eigenvectors, the best approximation is taking the first $K$ PCs\n",
    "\n",
    "> $\\displaystyle  C \\approx \\sum_{k=1}^K\\ \\lambda_k\\left(\\hat{e}_k\\,\\hat{e}_k^T\\right) =  E_K\\Lambda_KE_K^T$\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "### What is New Coordiante System?\n",
    "\n",
    "$E$ is a collection of $[\\hat{e}_1, \\dots, \\hat{e}_N]$\n",
    "\n",
    "- The $E$ matrix of eigenvectors is just a rotation, $E\\,E^T = I$ \n",
    "\n",
    "\n",
    "> $\\displaystyle  Z = E^T\\, X $\n",
    "\n",
    "\n",
    "- A truncated set of eigenvectors $E_K$ defines a projection\n",
    "\n",
    "> $\\displaystyle  Z_K = E_K^T\\, X $\n",
    "\n",
    "> and\n",
    "\n",
    "> $\\displaystyle  X_K = E_K E_K^T\\, X = P_K\\,X $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Sample Variance Note\n",
    "\n",
    "- Set of $N$-vectors with expectation value 0 arranged in $X=\\left[x_1, x_2, \\dots\\right]$ <br>\n",
    "<font color=\"red\">*This is NOT the random variable we talked about previously but the data matrix!*</font>\n",
    "\n",
    "> Sample covariance matrix is\n",
    "\n",
    ">$\\displaystyle C \\propto X X^T = \\sum_i x_i x_i^T$\n",
    "\n",
    "- Singular Value Decomposition (SVD) (方法2 np.linalg.svd(C))\n",
    "\n",
    ">$\\displaystyle X = U W V^T$\n",
    "\n",
    "> where $U^TU=I$, $W$ is diagonal, and $V^TV=I$\n",
    "\n",
    "- Hence \n",
    "\n",
    ">$\\displaystyle C \\propto UWV^T\\ VWU^T = U W^2 U^T$\n",
    "\n",
    "> So if $C=E\\Lambda E^T$, then $E = U$ and $ \\Lambda \\propto W^2$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using variance to explain the data\n",
    "\n",
    "- The eigenvectors of largest eigenvalues capture the most variance\n",
    "\n",
    "> $\\displaystyle  C \\approx C_K = \\sum_{k=1}^K\\ \\lambda_k\\left(\\hat{e}_k\\,\\hat{e}_k^T\\right) = \\sum_{k=1}^K\\ \\lambda_k\\,P_k$\n",
    "\n",
    "- And the remaining eigenvectors span the subspace with the least variance\n",
    "\n",
    "> $\\displaystyle  C - C_K = %\\sum_{l=K+1}^N\\ \\lambda_l\\left(\\hat{e}_l\\,\\hat{e}_l^T\\right) =\n",
    "\\sum_{l=K+1}^N\\ \\lambda_l\\,P_l$\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extra note  \n",
    "\n",
    "### Eigendecompsition with indices version \n",
    "\n",
    "\n",
    "> $\\displaystyle \\max_{a\\in{}\\mathbb{R}^N}  \\left[ \\sum_{i,j} a_i C_{ij} a_j - \\lambda\\,\\left(\\sum_i a_i^2 - 1\\right) \\right]$\n",
    "\n",
    "- Partial derivatives vanish at optimum\n",
    "\n",
    "> $\\displaystyle \\sum_{i,j} \\frac{\\partial a_i}{\\partial a_k} C_{ij} a_j + \\sum_{i,j} a_i C_{ij} \\frac{\\partial a_j}{\\partial a_k} - 2\\lambda\\,\\left(\\sum_i a_i \\frac{\\partial a_i}{\\partial a_k}\\right) = 0$ \n",
    "\n",
    "> $\\displaystyle \\sum_{i,j} \\delta_{ik} C_{ij} a_j + \\sum_{i,j} a_i C_{ij} \\delta_{jk} - 2\\lambda\\,\\left(\\sum_i a_i \\delta_{ik}\\right) = 0$ \n",
    "\n",
    "> $\\displaystyle \\sum_{j} C_{kj} a_j + \\sum_{i} a_i C_{ik}  - 2\\lambda\\,a_k = 0$ \n",
    "\n",
    "\n",
    "- Definition\n",
    "\n",
    ">$ \\delta_{kl} = \\left\\{ \\begin{array}{ll}\n",
    "         1 & \\mbox{if $k=l$}\\\\\n",
    "         0 & \\mbox{if $k\\neq{}l$}\\end{array} \\right.  $\n",
    "         \n",
    "- Useful to remember\n",
    "\n",
    ">$ \\displaystyle \\sum_l \\delta_{kl}\\,a_l = a_k$  \n",
    "\n",
    "> Cf. identity matrix:\n",
    ">$ I\\, \\boldsymbol{a} = \\boldsymbol{a}$ \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading\n",
    "\n",
    "- ## Best time using PCA (by PIERIAN DATA)\n",
    "\n",
    "PCA is an unsupervised linear dimensionality reduction algorithm to find a more meaningful basis or coordinate system for our data and works based on covariance matrix to find the strongest features if your samples .\n",
    "\n",
    "Its is used When we need to tackle the curse of dimensionality among data with linear relationships , i.e. where having too many dimensions (features) in your data causes noise and difficulties (it can be sound, picture or context). This specifically get worst when features have different scales (e.g. weight,length,area,speed, power, temperature,volume,time,cell number, etc. )\n",
    "\n",
    "We do this by reducing the dimension i.e. the features . But when should we reduce or change dimensions?\n",
    "\n",
    "1- Better Perspective and less Complexity: When we need a more realistic perspective and we have many features on a given data set and specifically when we have this intuitive knowledge that we don’t need this much number of features.\n",
    "\n",
    "Similarly, in many other practices modelling is easier in 2D than 3D , right?\n",
    "\n",
    "2 - Better visualization: When we cannot get a good visualization due to high number of dimensions we use PCA to reduce it into a shadow of 2D or 3D features (or even more but convenient enough for better parallel coordinates or Andrew Curve, e.g. when you transfer 100 features into 10 features you cannot still depict it as 2D or 3D but you can get a much better Andrew Curve)\n",
    "\n",
    "3- Reduce size: When we have too much data and we are going to use process-intensive algorithms (like many supervised algorithms) on the data so we need to get rid of redundancy .\n",
    "\n",
    "Sometimes change of perspective matters more than D reduction and we want to exploit dimensionality :\n",
    "\n",
    "4- Different perspective: Maybe you don’t have any of these motivations but you merely need to improve your knowledge on your data. CPA can give you the best linearly independent and different combinations of features so you can use to describe your data differently.\n",
    "\n",
    "\n",
    "http://blog.explainmydata.com/2012/07/should-you-apply-pca-to-your-data.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
